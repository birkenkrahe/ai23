<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>What is AI?</title>
<meta name="author" content="Marcus Birkenkrahe"/>
<meta name="description" content=""/>
<meta name="keywords" content=""/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js/dist/reveal.css"/>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js/dist/theme/black.css" id="theme"/>

</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide"><h1 class="title">What is AI?</h1><p class="subtitle">Seminar on Artificial Intelligence Fall 2023</p>
<h2 class="author">Marcus Birkenkrahe</h2><p class="date">Created: 2023-08-24 Thu 22:05</p>
</section>
<section>
<section id="slide-orga4febc3">
<h2 id="orga4febc3">What're you going to learn?</h2>
<ul>
<li>What is intelligence?</li>
<li>Different approaches to AI</li>
<li>The standard model of AI</li>
<li>Bounded rationality</li>
<li>The Value alignment problem</li>
<li>Asimov's Robot Laws</li>
<li>What's next?</li>

</ul>

</section>
</section>
<section>
<section id="slide-org4199adb">
<h2 id="org4199adb">What is intelligence?</h2>

<div id="org10c6b09" class="figure">
<p><img src="../img/intelligence.gif" alt="intelligence.gif" height="500px" />
</p>
</div>

</section>
<section id="slide-orgcb39268">
<h3 id="orgcb39268">Search patterns</h3>

<div id="org9ca0761" class="figure">
<p><img src="../img/googletrends.png" alt="googletrends.png" width="600px" />
</p>
</div>

</section>
<section id="slide-org118df7b">
<h3 id="org118df7b">Group work</h3>

<div id="org1d15fcd" class="figure">
<p><img src="../img/groupwork.gif" alt="groupwork.gif" height="200px" />
</p>
</div>

<ul>
<li>Get together in groups of 2-3</li>
<li>Define INTELLIGENCE (5')</li>
<li>Define ARTIFICIAL INTELLIGENCE (5')</li>
<li>Briefly present your results (10')</li>

</ul>

</section>
</section>
<section>
<section id="slide-org8c77aad">
<h2 id="org8c77aad">Different approaches to AI</h2>

<div id="orgc593298" class="figure">
<p><img src="../img/fields.gif" alt="fields.gif" height="300px" />
</p>
</div>

<p>
Which fields of inquiry (= disciplines) to use?
</p>

</section>
<section id="slide-org8f19add">
<h3 id="org8f19add">Fields of systematic inquiry</h3>

<div id="orgdb99da6" class="figure">
<p><img src="../img/fields.gif" alt="fields.gif" height="200px" />
</p>
</div>

<ul>
<li>Language</li>
<li>Philosophy</li>
<li>Science</li>
<li>History</li>

</ul>

</section>
<section id="slide-org856ed94">
<h3 id="org856ed94">Fundamental questions</h3>

<div id="org7a004db" class="figure">
<p><img src="../img/humanmachine.jpg" alt="humanmachine.jpg" width="500px" />
</p>
</div>

<ul>
<li>Should we focus on humans?</li>
<li>Should we focus on machines?</li>

</ul>

</section>
<section id="slide-orgb3f1631">
<h3 id="orgb3f1631">Four approaches</h3>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">THOUGHT / LOGIC</th>
<th scope="col" class="org-left">BEHAVIOR / ACTION</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">HUMANITY</td>
<td class="org-left">Cognitive modeling</td>
<td class="org-left">Turing Test</td>
</tr>

<tr>
<td class="org-left">RATIONALITY</td>
<td class="org-left">Laws of Thought</td>
<td class="org-left">Rational Agents</td>
</tr>
</tbody>
</table>

</section>
<section id="slide-org0c9f6b7">
<h4 id="org0c9f6b7">Four scenarios</h4>

<div id="org4a7ed55" class="figure">
<p><img src="../img/approaches1.png" alt="approaches1.png" height="500px" />
</p>
</div>

</section>
<section id="slide-orgdbf8b76">
<h4 id="orgdbf8b76">Acting humanly ("Turing test" approach)</h4>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">Natural language processing</td>
</tr>

<tr>
<td class="org-left">Knowledge representation</td>
</tr>

<tr>
<td class="org-left">Automated reasoning</td>
</tr>

<tr>
<td class="org-left">Machine learning</td>
</tr>

<tr>
<td class="org-left">Computer vision</td>
</tr>

<tr>
<td class="org-left">Robotics</td>
</tr>
</tbody>
</table>

</section>
<section id="slide-orgb11a0a3">
<h4 id="orgb11a0a3">Thinking humanly ("cognitive modeling" approach)</h4>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">Introspection</td>
</tr>

<tr>
<td class="org-left">Psychological experiments</td>
</tr>

<tr>
<td class="org-left">Brain imaging</td>
</tr>

<tr>
<td class="org-left">Cognitive science</td>
</tr>

<tr>
<td class="org-left">Algorithms</td>
</tr>
</tbody>
</table>

</section>
<section id="slide-org253e048">
<h4 id="org253e048">Thinking rationally ("laws of thought" approach)</h4>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">Syllogistic reasoning</td>
</tr>

<tr>
<td class="org-left">Logic</td>
</tr>

<tr>
<td class="org-left">Expert systems</td>
</tr>

<tr>
<td class="org-left">Uncertainty</td>
</tr>

<tr>
<td class="org-left">Probability</td>
</tr>
</tbody>
</table>

</section>
<section id="slide-orge818c2a">
<h4 id="orge818c2a">Acting rationally ("rational agent" approach)</h4>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">Combination approach</td>
</tr>

<tr>
<td class="org-left">Constructivist</td>
</tr>

<tr>
<td class="org-left">Doing the right thing</td>
</tr>

<tr>
<td class="org-left">Standard model</td>
</tr>

<tr>
<td class="org-left">Control theory</td>
</tr>
</tbody>
</table>

</section>
<section id="slide-org7788d51">
<h3 id="org7788d51">Major issues</h3>

<div id="orgad39315" class="figure">
<p><img src="../img/issues.gif" alt="issues.gif" height="200px" />
</p>
</div>

<ul>
<li>Bounded Rationality</li>
<li>Value alignment problem</li>

</ul>

</section>
<section id="slide-org26a9a85">
<h3 id="org26a9a85">Bounded rationality</h3>

<div id="org6eb720e" class="figure">
<p><img src="../img/bakopoulos.png" alt="bakopoulos.png" width="300px" />
</p>
</div>

<p>
Image: <a href="#/slide-orgfcdc5e6">Bakopoulos, 1985</a>
</p>

<blockquote >
<p>
AIMA: "For perfect rationality, the computational demands are just
too high."
</p>
</blockquote>

<aside class="notes">
<p>
The article by <a href="#/slide-orgfcdc5e6">Bakopoulos (1985)</a> helped to move IT from an arcane
discipline for technologists and nerds to a mainstream service
industry. To do this, Bakopoulos capitalized on the notion that
humans are not perfectly rational, that their rationality is
bounded by their humanity: "Data has no value unless put in the
context of the appropriate models, a process taxing the human
capacities to communicate, memorize and process information, and
thus leading to <i>bounded rationality</i>, which is a central concept
in organizational behavior theory."
</p>

<p>
I worked through this article a while back for a lecture I was
preparing, and found it remarkably difficult for such an old paper,
with a lot of connections in different directions - technology,
business, information theory, and philosophy.
</p>

<p>
Bakopoulos' result that information systems have no (or only
little) value without being properly integrated into business, and
that IT is useless if it does not 'speak" to people in ways that
they can understand and with results that they can measure, is
wonderfully relevant for the future development of AI, too.
</p>

</aside>

</section>
<section id="slide-org6af4ff8">
<h3 id="org6af4ff8">Value alignment</h3>

<div id="org64d23ec" class="figure">
<p><img src="../img/mechanicalturk.png" alt="mechanicalturk.png" width="200px" />
</p>
</div>

<p>
Image: <a href="https://www.amazon.com/Turk-Famous-Eighteenth-Century-Chess-Playing-Machine/dp/B000HWZ28Q">The Mechanical Turk</a>
</p>

<blockquote >
<p>
AIMA: "The values or objectives put into the machine must be
aligned with those of the human."
</p>
</blockquote>

<aside class="notes">
<p>
This kind of "alignment" sounds like an engineering task, but
actually it is a lot more complicated (or actually complex in a
technical sense): if rational agents are supposed to "do the right
thing", then their actions have to be not just logical, but
appropriate and moral. There is no algorithm for that - both
appropriateness (e.g. to a situation) and morality depend on the
circumstances. Take the example of Grace, the "ultra-lifelike
robotic nurse" from Hanson Robotics: she was designed for a certain
set of circumstances and both by, and for a particular set of moral
values. Will she do equally well in Japan and in Belgium, the
country with the "<a href="https://www.pbs.org/newshour/show/right-die-belgium-inside-worlds-liberal-euthanasia-laws">world's most liberal euthanasia law</a>", and
therefore possibly a different approach to caring for the elderly?
</p>

<p>
The example of an 'unethical rational agent' chosen by AIMA, which
relates to the "Mechanical Turk", a chess machine that was operated
by a human hidden inside the machine, is admittedly a lot less
critical than when taking care of the sick and elderly is at
stake. However, the age of AI that dazzled us by beating chess
champions is long behind us, and the age of robots like Grace is
upon us!
</p>

</aside>

</section>
<section id="slide-org74a4544">
<h3 id="org74a4544">Pros and cons</h3>

<div id="org38fb030" class="figure">
<p><img src="../img/groupwork.gif" alt="groupwork.gif" height="200px" />
</p>
</div>

<ul>
<li>Get together in groups of 2-3</li>
<li>Each group covers one approach</li>
<li>List pros and cons of your approach</li>
<li>Put your results <a href="https://ideaboardz.com/for/AI%20approaches%20pros%20&amp;amp;%20cons/4063343">on the Kanban board</a></li>

</ul>

</section>
<section id="slide-org849c710">
<h3 id="org849c710"><a href="https://en.wikipedia.org/wiki/Three_Laws_of_Robotics">Asimov's robot laws</a></h3>

<div id="org9127fc1" class="figure">
<p><img src="../img/asimov.jpg" alt="asimov.jpg" height="400px" />
</p>
</div>

<p>
Image: cover of "I, Robot" by Isaac Asimov (1940)
</p>

</section>
<section id="slide-orge984535">
<h4 id="orge984535">Which approach fits these laws best?</h4>
<ol>
<li>A robot may not injure a human being or, through inaction, allow
a human being to come to harm.</li>
<li>A robot must obey the orders given it by human beings except
where such orders would conflict with the First Law.</li>
<li>A robot must protect its own existence as long as such
protection does not conflict with the First or Second Law.</li>

</ol>

</section>
</section>
<section>
<section id="slide-orgc61aaab">
<h2 id="orgc61aaab">What's next?</h2>

<div id="org73cd7b6" class="figure">
<p><img src="../img/river.gif" alt="river.gif" height="300px" />
</p>
</div>

<ul>
<li>Scientific foundations of AI</li>
<li>History of AI</li>

</ul>

</section>
</section>
<section>
<section id="slide-org8433c30">
<h2 id="org8433c30">Any questions?</h2>

<div id="org7ca8716" class="figure">
<p><img src="../img/thankyou.gif" alt="thankyou.gif" height="400px" />
</p>
</div>

<p>
<a href="https://github.com/birkenkrahe/ai482/tree/main/2_what_is_ai">This presentation is available online.</a>
</p>

</section>
</section>
<section>
<section id="slide-org4504128">
<h2 id="org4504128">References</h2>
<p>
<a id="orgfcdc5e6"></a> Bakopoulos, J. Yannis, "Toward a More Precise
Concept of Information Technology" (1985). ICIS 1985 Proceedings. 4.
<a href="http://aisel.aisnet.org/icis1985/4">http://aisel.aisnet.org/icis1985/4</a>
</p>
</section>
</section>
</div>
</div>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/dist/reveal.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/plugin/markdown/markdown.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/plugin/zoom/zoom.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/plugin/notes/notes.js"></script>


<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({
plugins: [RevealMarkdown, RevealZoom, RevealNotes],
transition: 'cube'
});

</script>
</body>
</html>
